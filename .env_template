# ---------- API KEYS ---------- #
OPENAI_API_KEY = ""
OPENROUTER_API_KEY=""
GEMINI_API_KEY=""



# ---------- MILVUS ---------- #
MILVUS_URL="http://localhost:19530"
MILVUS_DATABASE_NAME="fAIth"
MILVUS_USERNAME="root"
MILVUS_PASSWORD="Milvus"

# Use only vector embeddings with your embedding model
# DATABASE_TYPE = "dense"
# Use only lexical search with BM25
# DATABASE_TYPE = "sparse"
# Use both vector embeddings and lexical search
DATABASE_TYPE = "hybrid"

# How much influence the vector embeddings and lexical search has on the overall results
# Only available when using "hybrid" search
DENSE_WEIGHT=0.8
SPARSE_WEIGHT=0.2



# ---------- EMBEDDING MODEL ---------- #
# EMBEDDING_MODEL_RUNNER = "vllm"
EMBEDDING_MODEL_RUNNER = "llama_cpp"
# EMBEDDING_MODEL_RUNNER = "ollama"
# EMBEDDING_MODEL_RUNNER = "docker_model_runner"

EMBEDDING_MODEL_ID = "unsloth/Qwen3-0.6B-GGUF:Q4_K_M"
# EMBEDDING_MODEL_ID = "Qwen/Qwen3-0.6B"
EMBEDDING_MODEL_DOCUMENT_PROMPT="{text}"
EMBEDDING_MODEL_QUERY_PROMPT="Instruct: Given a Bible-related query, retrieve relevant passages that answer the query.\nQuery: {text}"

EMBEDDING_DEVICE="cuda"

# Llama CPP Python specific things
EMBEDDING_MODEL_REPO = ""
EMBEDDING_GPU_LAYERS= -1 # How many layers you want allocated to the GPU. 0 for none (all to CPU), -1 for all possible layers on GPU

# vLLM specific things
ENFORCE_EAGER = False

