# ---------- BIBLE CONFIG ---------- #
DEFAULT_VERSION = "bsb"
DEFAULT_BOOK = "Genesis"
DEFAULT_CHAPTER = 1



# ---------- API KEYS ---------- #
OPENAI_API_KEY = ""
OPENROUTER_API_KEY = ""
GEMINI_API_KEY = ""
HF_TOKEN = ""



# ---------- MILVUS ---------- #
MILVUS_URL = "http://localhost"
MILVUS_PORT = 19530
MILVUS_DATABASE_NAME = "fAIth"
MILVUS_USERNAME = "root"
MILVUS_PASSWORD = "Milvus"

# Use only vector embeddings with your embedding model
# DATABASE_TYPE = "dense"
# Use only lexical search with BM25
# DATABASE_TYPE = "sparse"
# Use both vector embeddings and lexical search
DATABASE_TYPE = "hybrid"

# How much influence the vector embeddings and lexical search has on the overall results
# Only available when using "hybrid" search
DENSE_WEIGHT = 0.8
SPARSE_WEIGHT = 0.2



# ---------- EMBEDDING MODEL ---------- #
# Endpoints
# /v1/ is automatically appended
EMBEDDING_URL = "http://localhost"
EMBEDDING_PORT = 11435

# Embedding Model Runners
# EMBEDDING_MODEL_RUNNER = "vllm"
EMBEDDING_MODEL_RUNNER = "llama_cpp"
# EMBEDDING_MODEL_RUNNER = "ollama"
# EMBEDDING_MODEL_RUNNER = "docker_model_runner"
# EMBEDDING_MODEL_RUNNER = "sglang"

# GPU Type
EMBEDDING_GPU_TYPE="cpu"
# EMBEDDING_GPU_TYPE="nvidia"
# EMBEDDING_GPU_TYPE="amd"
# EMBEDDING_GPU_TYPE="intel"

# GPU Driver
EMBEDDING_DRIVER ="cpu"
# EMBEDDING_DRIVER ="cuda"
# EMBEDDING_DRIVER ="rocm"
# EMBEDDING_DRIVER ="vulkan"

# Embedding Model
EMBEDDING_MODEL_ID = "Qwen/Qwen3-Embedding-0.6B-GGUF:Q8_0" # The model ID you want to use for the embedding model
EMBEDDING_MODEL_DOCUMENT_PROMPT = "{text}"
EMBEDDING_MODEL_QUERY_PROMPT = "Instruct: Given a Bible-related query, retrieve relevant passages that answer the query.\nQuery: {text}"
EMBEDDING_MAX_CONTEXT_LENGTH = 4096 # The maximum context length you want to allow for the embedding model

# Llama CPP Python specific things
EMBEDDING_LLAMA_CPP_GPU_LAYERS = -1 # How many layers you want allocated to the GPU. 0 for none (all to CPU), -1 for all possible layers on GPU
EMBEDDING_LLAMA_CPP_CONCURRENCY = 1 # How many concurrent requests you want to allow to the embedding model

# vLLM specific things
EMBEDDING_VLLM_ENFORCE_EAGER = False



# ---------- LLM MODEL ---------- #
# Endpoints
# /v1/ is automatically appended
LLM_URL = "http://localhost"
LLM_PORT = 11436

# LLM Model Runners
# LLM_MODEL_RUNNER = "vllm"
LLM_MODEL_RUNNER = "llama_cpp"
# LLM_MODEL_RUNNER = "ollama"
# LLM_MODEL_RUNNER = "docker_model_runner"
# LLM_MODEL_RUNNER = "sglang"

# GPU Type
LLM_GPU_TYPE="cpu"
# LLM_GPU_TYPE="nvidia"
# LLM_GPU_TYPE="amd"
# LLM_GPU_TYPE="intel"

# GPU Driver
LLM_DRIVER="cpu"
# LLM_DRIVER="cuda"
# LLM_DRIVER="rocm"
# LLM_DRIVER="vulkan"

# LLM Model
LLM_MODEL_ID = "unsloth/Qwen3-4B-Instruct-2507-GGUF:Q4_K_M" # The model ID you want to use for the LLM model
LLM_MAX_CONTEXT_LENGTH = 4096 # The maximum context length you want to allow for the LLM model

# Llama CPP Python specific things
LLM_LLAMA_CPP_GPU_LAYERS = -1 # How many layers you want allocated to the GPU. 0 for none (all to CPU), -1 for all possible layers on GPU
LLM_LLAMA_CPP_CONCURRENCY = 1 # How many concurrent requests you want to allow to the LLM model

# vLLM specific things
LLM_VLLM_ENFORCE_EAGER = False