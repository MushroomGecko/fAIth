# ---------- API KEYS ---------- #
OPENAI_API_KEY = ""
OPENROUTER_API_KEY=""
GEMINI_API_KEY=""



# ---------- MILVUS ---------- #
MILVUS_URL="http://localhost:19530"
MILVUS_DATABASE_NAME="fAIth"
MILVUS_USERNAME="root"
MILVUS_PASSWORD="Milvus"

# Use only vector embeddings with your embedding model
# DATABASE_TYPE = "dense"
# Use only lexical search with BM25
# DATABASE_TYPE = "sparse"
# Use both vector embeddings and lexical search
DATABASE_TYPE = "hybrid"

# How much influence the vector embeddings and lexical search has on the overall results
# Only available when using "hybrid" search
DENSE_WEIGHT=0.8
SPARSE_WEIGHT=0.2



# ---------- EMBEDDING MODEL ---------- #
EMBEDDING_MODEL_RUNNER = "hf_sentence_transformers"
# EMBEDDING_MODEL_RUNNER = "llama_cpp"
# EMBEDDING_MODEL_RUNNER = "ollama"
# EMBEDDING_MODEL_RUNNER = "docker_model_runner"

EMBEDDING_MODEL_ID = "Qwen/Qwen3-Embedding-0.6B"
EMBEDDING_MODEL_DOCUMENT_PROMPT="{text}"
EMBEDDING_MODEL_QUERY_PROMPT="Instruct: Given a Bible-related query, retrieve relevant passages that answer the query.\nQuery: {text}"

# Llama CPP Python and HF Transformers
# DEVICE="cpu"
EMBEDDING_DEVICE="cuda"

# Llama CPP Python specific things
EMBEDDING_MODEL_REPO = ""
# The GPU_LAYERS variable is used in the Llama CPP Python runner to specify the number of layers to offload to the GPU
# This is only used if the DEVICE is set to "cuda", otherwise the number of GPU_LAYERS is automatically 0
EMBEDDING_GPU_LAYERS=32
